# 爬虫本质

模拟浏览器打开网页，获取网页中我们想要的那部分数据
其实搜索引擎就是先爬取各个网页内容，然后放在自己的数据库里面，创建索引，  
在索引中进行分类、归档、排序，然后将结果返回给用户

## 爬虫的过程

+ 准备工作
  通过浏览器查看分析目标网页
+ 获取数据
  通过HTTP库向目标占点发起请求，请求可以包含额外的header等信息，如果服务器能正常响应，会得到一个Response这便是所要获取的页面内容
  - request
  - urlib
  - 模拟浏览器: selenium, 或者 PhantomJS
+ 解析内容
  得到的内容可能是HTML和json等格式，可以用页面解析式、正则表达式等进行解析
  - 正则
  - BeautifulSoup
  - lxml
+ 保存数据
  保存形式多种多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件  
  - 存入 txt, csv 等文件
  - 存入mysql, mongodb等数据库

还有些爬虫的框架，例如：Scrapy

### http常见的状态码
    
    200 OK    
    一切正常
    301 Moved Permanently
    客户请求的文档在其他地方，新的URL在Location头中给出，浏览器应该自动地访问新的URL。
    302 Found
    类似于301，但新的URL应该被视为临时性的替代，而不是永久性的。
    304 Not Modified
    请求的资源未更新
    400 Bad Request
    非法请求
    401 Unauthorized
    请求未经授权
    403 Forbidden
    禁止访问
    404 Not Found
    没有找到对应页面
    500 Internal Server Error
    服务器内部出现错误
    501 Not Implemented
    服务器不支持实现请求所需要的功能

### 如何分析页面
借助`F12`来分析网页
+ 在Elements下找到需要的数据位置
在开发者模式下，点击左上角小箭头后点击页面中的文字，我们可以找到我们想要爬取的内容的位置
+ 点击`Network`
下面有很多小线条，这是时间线，表示你点击网页后，浏览器向服务器发出了这么多次请求，会发现有很多线条，意味着有很多次交互  
你可以点击左上角停止按钮，点击之后可以慢慢分析浏览器和服务器之间的交互  
可能会找到status是200，根据上面http常见状态码，可以知道表示一切正常，服务器正常接收浏览器请求然后返回文件  
左边的Name即是浏览器发出的请求，点击其中一个请求，可以显示请求的具体内容  
点击Headers  
需要注意的一点是  
Response Headers 是浏览器发给服务器的，并不是服务器发给我们的
浏览器通过Response Headers告诉服务器返回的内容应该适配什么条件
而服务器真正返回给浏览器的是在 Response 里面

+ 来看Headers的内容  

Headers内容是让对方服务器在接收到请求的时候能够了解我们的状态
比如说可以知道我们是什么浏览器    
通过User-Agent可以知道我们的浏览器的类型，换句话说，可以知道我们的浏览器可以接收什么水平的消息，比如说IE浏览器，有些网页在IE6上显示结果非常糟糕，不能兼容，而在IE8上面就能正常显示
服务器可以看到我的win10操作系统，看到底层架构比如X64,具体浏览器版本号是chrome  
甚至能够看到cookie  

+ cookie是什么？ -->由用户客户端计算机暂时或永久保存的信息  

是服务器为了能够表示每个客户端，无论是登录信息，还是客户端的一些行为信息而保存在本地的一些内容，而且这个内容通常是加密的,这个信息可能是我的地理位置，我的IP地址，我的以前浏览的关键字或者我是从哪一个网页进入到当前网页都会隐藏在cookie里面  
所以说像淘宝这样的网页会知道你的信息，它们知道可能不单单是通过你的本地，还有可能是通过cookie解析

    General
    Request URL : http://   发送请求的路径
    Request Methon : GET    请求的方式是  get
    Status Code : 200 OK    请求的状态是  200 OK 表示一切正常

    Response Headers       仍然是我们通过浏览器发送给服务器的，告诉服务器返回内容的格式，真正返回内容在Response中
    User-Agent 在Response Headers下面  

+ 需要掌握的点
 - 一是`User-Agent`表明我们是什么版本的浏览器，如果没有可能不会得到返回信息
 - `cookie` 如果想要做一些登录以后才能看到一些内容，想把这个内容爬取下来，就必须学会怎么存储`cookie`和怎么读取`cookie`，因为没有cookie就认为你没有登录
 
### 经常用到的库
```python
import bs4  # 网页解析，获取数据  Beautiful Soup
import re   # 正则表达式，进行文字匹配
import urllib.request,urllib.error  # 制订URL,获取网页数据
import xlwt #进行excel操作
import sqlite3  # 进行SQLite数据库操作
```






